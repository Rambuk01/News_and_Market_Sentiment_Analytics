---
title: "News and Market Sentiment Analytics"
subtitle: "Lecture 3: Classification pt 1"
author: 'Christian Vedel,<br> Department of Economics<br><br>
Email: christian-vs@sam.sdu.dk'
date: "Updated `r Sys.Date()`" 
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=TRUE, include=TRUE, cache=TRUE)
library(reticulate)
use_condaenv("sentimentF23")
```

```{python include=FALSE}
import nltk
```


```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
```

# Last time
.pull-left[
- Lexical Resources
- Finn Årup Nielsen 
- Basic workings of NLTK
- Coding challenge: Working with Text Data
]

.pull-right[
![Trees](Figures/Trees.jpg)
]

---
# Today's lecture
.pull-left[
- We can understand many NLP tasks as classification tasks
- Today mainly 'structural' classification 
- Based on understanding structure of language
- spaCy library 
- Zero-shot classificaiton using Transformers
]

.pull-right[
![Trees](Figures/Trees.jpg)
]

---
# A look insight WordNet

.pull-left[
```{python}
from nltk.corpus import wordnet as wn

# Synonomym names
wn.synsets('motorcar')

# Synonyms
wn.synset('car.n.01').lemma_names()
```
]


.pull-right[
![Wordnet](https://www.nltk.org/images/wordnet-hierarchy.png)
]

#### Example
> the boy's cars are different colors 
> $\Rightarrow$ the boy car be differ color

---
# spaCy

.red[### Why is it we don't just stick to NLTK?]

.pull-left[

1. spaCy is a streamlined NLP library
  
2. Language Models: spaCy vs. NLTK:
  + spaCy provides pre-trained language models
  + NLTK is more flexible
  
3. Efficiency and Ease of Use: spaCy vs. NLTK:

]


.pull-right[
![spaCy](Figures/spaCy.png)
]

---
# Tokenization in NLTK
```{python}
# Import NLTK and download relevant resources (if not already done in previous lectures)
import nltk

# Download the NLTK stopwords corpus
nltk.download("stopwords")

# Example: Tokenization using NLTK (recap)
from nltk.tokenize import word_tokenize

text = "Natural language processing is a subfield of artificial intelligence."
tokens = word_tokenize(text)
print(tokens)

```

---
# Tokenizaiton in spaCy
```{python}
# Import spaCy and load the English language model
import spacy

nlp = spacy.load("en_core_web_sm")

# Tokenization using spaCy
text = "spaCy is a powerful NLP library."
doc = nlp(text)

# Extract tokens using spaCy
tokens = [token.text for token in doc]
print(tokens)

```


---
# The OG NLP classification problem

.pull-left[
- We want to identify parts of speach
- One simple apporach is ***Part of Speach***-tagging (*POS*) 
- spaCy does this at a high level based on an underlying model. Here [*en_core_web_sm*](https://spacy.io/models/en)
]

.pull-right[
.small123[
```{python echo=FALSE}
# Import spaCy and load the English language model
import spacy
nlp = spacy.load("en_core_web_sm")
```


```{python}
text = "The quick brown fox jumps over the lazy dog."
doc = nlp(text)

# Extract tokens and their part-of-speech tags using spaCy
pos_tags = [(token.text, token.pos_) for token in doc]

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(foreach)
library(tidyverse)
foreach(i = py$pos_tags, .combine = "rbind") %do% {
  x = unlist(i)
  names(x) = c("Token", "Tag")
  x
} %>% 
  data.frame() %>%
  remove_rownames() %>% 
  knitr::kable()
```

]
]

---
# NLTK solution

```{python}
from nltk import word_tokenize, pos_tag

# Sample sentence
sentence = "NLTK is a leading platform for building Python programs."

# Tokenize the sentence
words = word_tokenize(sentence)

# Perform PoS tagging
tags = pos_tag(words)

# Print the PoS tags
print(tags)

```


---
## References (1/2)

.small123[
Andersen, H. C. (1844). The Nightingale. https://www.hcandersen-homepage.dk/?page_id=2257

Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Descartes, R. (1637). Discourse on the Method of Rightly Conducting One's Reason and of Seeking Truth in the Sciences (J. Veitch, Trans.). EBook-No. 59. https://www.gutenberg.org/ebooks/59

Firth, J.R. (1957). "A synopsis of linguistic theory 1930–1955". Studies in Linguistic Analysis: 1–32. Reprinted in F.R. Palmer, ed. (1968). Selected Papers of J.R. Firth 1952–1959. London: Longman.

Gurnee, W., & Tegmark, M. (2023). Language Models Represent Space and Time. https://arxiv.org/abs/2310.02207

Pearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62(3), 54–60. https://doi.org/10.1145/3241036

Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training. https://api.semanticscholar.org/CorpusID:49313245

Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386–408. https://doi.org/10.1037/h0042519
]


---
## References (2/2)
.small123[
Roweis, S. T., Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". Science, 290(5500), 2323–6. https://doi.org/10.1126/science.290.5500.2323

Searle, J. (1980), "Minds, Brains and Programs", Behavioral and Brain Sciences, 3(3), 417–457. https://doi.org/10.1017/S0140525X00005756

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need. https://arxiv.org/abs/1706.03762

Weizenbaum, J. (1966). ELIZA—a Computer Program for the Study of Natural Language Communication between Man and Machine. Commun. ACM, 9(1), 36–45. https://doi.org/10.1145/365153.365168
]

