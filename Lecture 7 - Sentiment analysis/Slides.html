<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>News and Market Sentiment Analytics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Vedel,  Department of Economics   Email: christian-vs@sam.sdu.dk" />
    <script src="libs/header-attrs-2.21/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/xaringanExtra-progressBar-0.0.1/progress-bar.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# News and Market Sentiment Analytics
]
.subtitle[
## Lecture 7: Sentiment analysis
]
.author[
### Christian Vedel,<br> Department of Economics<br><br> Email: <a href="mailto:christian-vs@sam.sdu.dk" class="email">christian-vs@sam.sdu.dk</a>
]
.date[
### Updated 2023-12-04
]

---








<style>.xe__progress-bar__container {
  top:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #808080;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>

&lt;style type="text/css"&gt;
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
&lt;/style&gt;

# Last time
.pull-left[
- Distances
- Edit distances and its applicaitons 
- Semantic distances and its applicaitons 
- Under the hood of one-shot classificaiton 
]

.pull-right[
![Trees](Figures/Trees.jpg)
]

---
# Today's lecture
.pull-left[
- Four recent academic papers on sentiment analysis
- How to access LLaMA2
- Recommendations on sentiment analysis
]

.pull-right[
![Dataflow](Figures/0_Transformations, dataflows, magical, mathematical _esrgan-v1-x2plus.png)
]

---
# Types of Language Models

#### Conventional Language Models:
- Predict probability distributions over linguistic sequences.
- Developed through statistical or data-driven approaches.
- Examples include N-grams LMs, exponential LMs, and earlier neural LMs.

`$$P(u_1, u_2, \ldots, u_t) = P(u_1)P(u_2|u_1)P(u_3|u_1, u_2) \ldots P(u_t|u_1, \ldots, u_{t-1})$$`

#### Pretrained Language Models:
- Pretrained on a broad range of linguistic tasks and objectives.
- Mainstream approach in contemporary language modeling.
- Often based on transformers
- "This is a [MASK] type of food"

`$$P(u_t|u_{&lt;t}), \quad P(u_t|u_{&lt;t}, u_{&gt;t})$$`

.footnote[
.small123[[Wei, Wang, Wang, Kuo (2023)](https://arxiv.org/abs/2303.05759)]
]



---
# A list of LMs
.pull-left-wide[
- [BERT-base-uncased](https://huggingface.co/bert-base-uncased): 110M parameters; One of the most used LMs out there
  + [FinBERT](https://huggingface.co/ProsusAI/finbert): Finetuned for financial sentiment
- [XLM-RoBERTa](https://huggingface.co/xlm-roberta-base): 279M parameters; Trained on 100 languages, powers our [HISCO project](https://raw.githack.com/christianvedels/Presentations/main/HISCO/Slides_flash.html)

- [GPT-3.5](https://openai.com/pricing): 175 billion parameters; One of the largest language models:
  + Also GPT-4, rumored to have 1.76 trillion parameters

- [LLaMA2](https://huggingface.co/meta-llama/Llama-2-7b): 7B, 13B or 70B versions. Open source version of ChatGPT from Meta. You can run it locally (but it will take a while)

- [DistilBERT](https://huggingface.co/distilbert-base-uncased): 66M parameters; A distilled version of BERT for faster inference

- **Ancient result:** An LLM will often [implicitly model sentiment](https://openai.com/research/unsupervised-sentiment-neuron)
]

.pull-right-narrow[
![LLaMA_poly](Figures/LLAMA_poly.png)
.small123[*Dreamstudio.ai*]

.small123[*For more check Naveed et al. (2023)*]
]



---
# Pan et al. (2023)
.pull-left[
- *Target based sentiment analysis* in Spanish financial markets
- Pipeline:
  1. Identify entities (E.g. companies ) 
  2. Identify sentiment for target not just the whole text
  
- Accuracy in the 70s percent

]

.pull-right[
![Pan2023](Figures/Pan et al 2023.PNG)
]

---
# Plaue (2023)
.pull-left[
- Not an academic article :-)
- A nice go to to get started
- Goes over a few standard ways of doing sentiment analysis
- Main takeaways:
  + Finetuning of a transformer model is worth it 
  + But it might take a while to run for a lot of news
]

.pull-right[
![Plaue](Figures/Plaue2023.PNG)
.small123[*Finetuned LLM, figure from Plaue (2023)*]
]

---
# Zhang, Yang, Liu (2023)

- LLM do not understand numbers. Accounts, etc. can be key information in sentiment analysis 
- LLMs can be instructed/prompted to have certain behaviours
- *Instruction tuning* helps 
  + see Zhang et al. (2023) for state of the art on instruction tuning
- LLaMA-7B is used

![Instructiontuning](Figures/Instructiontuning.PNG)
.small123[*Figure 1 from Zhang, Yang, Liu (2023)*]

---
# Zhang et al. (2023)
.pull-left[
- Sometimes the problem is missing context 
- What if we could get an LLM to retrieve relevant context from the internet when applicable? 
- Extract important bits from texts "It was troubling what happened at **Apple** today" `\(\rightarrow\)` [Retrieve information from **Apple**]

#### Steps
1. Search for [text] on search engine 
2. Extract and add text to context if relevant
3. Use query + context with instruction tuning to get sentiment

]

.pull-right[
**Retrieval Augmented Generation**
![RAG](Figures/RAG.PNG)
.small123[*Table III from Zhang 2023*]
]

---
# LLaMA2
.pull-left[
- An open source version of 
- [Step by step guide for Tuning LLaMA2](https://www.datacamp.com/tutorial/fine-tuning-llama-2)
- [HugginFace Guide](https://huggingface.co/blog/llama2)
- [LLaMA2 demo](https://huggingface.co/spaces/huggingface-projects/llama-2-7b-chat)
![LLaMA2sent](Figures/LLAMA2_sentiment.PNG)

]

.pull-right[
![LLaMA2](Figures/Finetuning_LLAMA2.png)
.small123[*Finetuning LLaMA2 according to dreamstudio.ai*]
]

.footnote[
.small123[[See Touvron et. al 2023](https://arxiv.org/abs/2307.09288)]
]

---
# Why are people excited about LLaMA2?
.pull-left[
- It is an open source version of chatGPT
- It works well 
- We can freely implement it anywhere 
- Tough to finetune but can be done with Parameter-Efficient Fine-Tuning ([PEFT](https://github.com/huggingface/peft))
]

.pull-right[
![LLAMA](Figures/LLAMA_paper.png)
.small123[*dreamstudio.ai*]
]

---
# Where to get data
.pull-left[
- Twitter is suddenly expensive
- Reuters and Bloomberg: [Start with some of the compiled data out there](https://github.com/philipperemy/financial-news-dataset)
- [Thomson Reuters API](https://developers.thomsonreuters.com/pages/home)
- [Bloomberg API](https://www.bloomberg.com/professional/support/api-library/); [Bloomberg Python package](https://bloomberg.github.io/blpapi-docs/python/3.23/) 
- [All Danish News on Infomedia](https://infomedia.dk/mediearkiv/)
- [The Guardian](https://open-platform.theguardian.com/)
- [Many GitHub repositories for scraping news](https://github.com/topics/news-scraper)
]


.pull-right[
![Twitter](Figures/Tweets_price.PNG)
.small123[*[Tweet prices](https://developer.twitter.com/en/portal/petition/essential/basic-info)*]
]


---
# Timing is key
.pull-left[
- Remember the [*Efficient Market Hypothesis*](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%205%20-%20Understanding%20and%20utilizing%20grammar/Slides.html?panelset=front-page&amp;panelset1=text#18)
- It is not valuable to know what changes are happening on the market **now** we need to know what changes will happen in **the future**
- **Implication:** Prediction must be done with a lag. I.e. predictions should reflect: `\(\Delta Y_t=f(\Delta Y_{t-1}, Z_{t-1}\)`
]

.pull-right[
![Stoncks](Figures/stonks.jpeg)
]

---
# Best practice data transformations 
.pull-left[
- Take logs of the outcome `\(Y_t=log(Stock\;price_t)\)` 
  + This makes 1% change comparable across scales. 
- Take differences:
  + `\(\Delta Y = Y_t - Y_{t-1}\)`
  + This centers the data around 0
- Regularization on the text-level:
  + Random word swapping/insertion
  + Random character swapping/insertion
  + Back translation
  + [TextAttack library](https://textattack.readthedocs.io/en/master/)
]



---
# Code example
- [Finetuning DistilBERT for IMDB sentiments](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%207%20-%20Sentiment%20analysis/Code/Training_sentiment.py) 
- [Running LLaMA2 on your own machine](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%207%20-%20Sentiment%20analysis/Code/Using_LLAMA2.py)

---
# Next time
.pull-left[
- Topic modelling: Clustering, LDA, etc. 
- Content of your choice 
]

.pull-right[
![Trees](Figures/Trees.jpg)
]



---
## References (1/2)

.small123[
Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., &amp; Bossan, B. (2022). PEFT: State-of-the-art Parameter-Efficient Fine-Tuning methods. Retrieved from https://github.com/huggingface/peft

Naveed, H., Khan, A. U., Qiu, S., Saqib, M., Anwar, S., Usman, M., Akhtar, N., Barnes, N., &amp; Mian, A. (2023). A Comprehensive Overview of Large Language Models. arXiv preprint arXiv:2307.06435. https://arxiv.org/abs/2307.06435

Plaue, M. (2023, May 5). Large-scale language models for innovation and technology intelligence: sentiment analysis on news articles. MAPEGY Tech. https://medium.com/mapegy-tech/large-scale-language-models-for-innovation-and-technology-intelligence-sentiment-analysis-on-news-2c1ed1f6f2ad

Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Xiang Kuan, J., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., &amp; Scialom, T. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288. https://arxiv.org/abs/2307.09288

Wei, C., Wang, Y.-C., Wang, B., &amp; Kuo, C.-C. J. (2023). An Overview on Language Models: Recent Developments and Outlook. arXiv preprint arXiv:2303.05759. https://arxiv.org/abs/2303.05759

]

---
## References (2/2)

.small123[
Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., &amp; Wang, G. (2023). Instruction Tuning for Large Language Models: A Survey. arXiv preprint arXiv:2308.10792. https://arxiv.org/abs/2308.10792

Zhang, B., Yang, H., Zhou, T., Babar, A., &amp; Liu, X.-Y. (2023). Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models. arXiv preprint arXiv:2310.04027. https://arxiv.org/abs/2310.04027

Zhang, B., Yang, H., &amp; Liu, X.-Y. (2023). Instruct-FinGPT: Financial Sentiment Analysis by Instruction Tuning of General-Purpose Large Language Models. arXiv preprint arXiv:2306.12659. https://arxiv.org/abs/2306.12659 

]



    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(SDU_logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 125px;
  height: 60px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // insert more to hide here
    ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
