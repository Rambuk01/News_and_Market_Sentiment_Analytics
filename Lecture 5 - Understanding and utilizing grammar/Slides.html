<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>News and Market Sentiment Analytics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Vedel,  Department of Economics   Email: christian-vs@sam.sdu.dk" />
    <script src="libs/header-attrs-2.25/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/panelset-0.2.6/panelset.css" rel="stylesheet" />
    <script src="libs/panelset-0.2.6/panelset.js"></script>
    <link href="libs/tile-view-0.2.6/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view-0.2.6/tile-view.js"></script>
    <script src="libs/xaringanExtra-progressBar-0.0.1/progress-bar.js"></script>
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# News and Market Sentiment Analytics
]
.subtitle[
## Lecture 5: Understanding and utilizing grammar
]
.author[
### Christian Vedel,<br> Department of Economics<br><br> Email: <a href="mailto:christian-vs@sam.sdu.dk" class="email">christian-vs@sam.sdu.dk</a>
]
.date[
### Updated 2023-12-03
]

---








<style>.xe__progress-bar__container {
  top:0;
  opacity: 1;
  position:absolute;
  right:0;
  left: 0;
}
.xe__progress-bar {
  height: 0.25em;
  background-color: #808080;
  width: calc(var(--slide-current) / var(--slide-total) * 100%);
}
.remark-visible .xe__progress-bar {
  animation: xe__progress-bar__wipe 200ms forwards;
  animation-timing-function: cubic-bezier(.86,0,.07,1);
}
@keyframes xe__progress-bar__wipe {
  0% { width: calc(var(--slide-previous) / var(--slide-total) * 100%); }
  100% { width: calc(var(--slide-current) / var(--slide-total) * 100%); }
}</style>


&lt;style type="text/css"&gt;
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
&lt;/style&gt;

# Last time
.pull-left[
- More standard classification approach
- Extracting features using NLP tools 
- Training data `\(\rightarrow\)` prediction engine
- **Research example:** Automatic occupational classification
]

.pull-right[
![Trees](Figures/Trees.jpg)
]

---
# Today's lecture
.pull-left[
- Using grammar
- Named entity recognition
- Basic financial theory
]

.pull-right[
![Misc](Figures/0_Transformations, dataflows, magical, mathematical _esrgan-v1-x2plus.png)
]


---
# Understanding and Utilizing Grammar

.pull-left[

- From bag(s) of words to sentence comprehension:

- Same words but very different meaning.

- One layer of abstraction below transformers.

![BoW](Figures/BagOfWords.png)

]

.pull-right[
.small123[
### Examples
&gt; a. *The scientists discovered a new species in the Amazon rainforest*  

&gt; b. *The research paper highlighted that scientists discovered a new species in the Amazon rainforest*  

&gt; c. *Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest*  

&gt; d. *I believe Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest*
]
]
---
# Revisiting Basic Grammatical Ideas

.pull-left-narrow[
.small123[
- **Noun**
  - *Person, place, thing, or idea*
    - *Example: The **cat** is on the roof.*
- **Verb**
  - *Action or state of being*
    - *Example: The dog **barks** loudly.*
- **Adjective**
  - *Describes a noun*
    - *Example: She has a **beautiful** garden.*
- **Adverb**
  - *Modifies a verb, adjective, or other adverbs*
    - *Example: He speaks **loudly** and **clearly**.*
]

]

.pull-right-wide[
.small123[

```python
from nltk import pos_tag
from nltk.tokenize import word_tokenize

sentence = "The cat is on the roof."
tokens = word_tokenize(sentence)
pos_tags = pos_tag(tokens, tagset = 'universal')
for word, tag in pos_tags: # Pretty print
    print(f"{word: &lt;15} {tag}")
```

```
## The             DET
## cat             NOUN
## is              VERB
## on              ADP
## the             DET
## roof            NOUN
## .               .
```
]

.red[
***Can be used to build understanding***
]

]

---
# Note: We can also subcategorize e.g. verbs

| Form  | Category                | Tag  |
|-------|-------------------------|------|
| go    | base                    | VB   |
| goes  | 3rd singular present   | VBZ  |
| gone  | past participle         | VBN  |
| going | gerund                  | VBG  |
| went  | simple past             | VBD  |

---
# Parts of Speech

- **Noun Phrases**
  - *Group of words centered around a noun*
    - *Example: **The old book** on the shelf is valuable.*
- **Verb Phrases**
  - *Group of words centered around a verb*
    - *Example: The cat **is sleeping** peacefully.*
- **Sentence Structure**
  - *Subject, Predicate, Object*
    - *Example: **She** (subject) **ate** (verb) **the delicious cake** (object).*


---
# Chunkers: Extracting Phrases
.small123[

```python
from nltk import pos_tag, RegexpParser
from nltk.tokenize import word_tokenize

sentence = "The cat is on the roof."
tokens = word_tokenize(sentence)
pos_tags = pos_tag(tokens)

# Chunking with a simple grammar
grammar = r"""
  NP: {&lt;DT&gt;?&lt;JJ&gt;*&lt;NN&gt;}  # NP: Noun Phrase
  VP: {&lt;VB.*&gt;&lt;DT&gt;?&lt;JJ&gt;*&lt;NN|IN&gt;}  # VP: Verb Phrase
"""
chunk_parser = RegexpParser(grammar)
chunks = chunk_parser.parse(pos_tags)

for subtree in chunks.subtrees():
    if subtree.label():
        print(subtree)
```

```
## (S (NP The/DT cat/NN) (VP is/VBZ on/IN) (NP the/DT roof/NN) ./.)
## (NP The/DT cat/NN)
## (VP is/VBZ on/IN)
## (NP the/DT roof/NN)
```
]
---
# Chunkers: Extracting Phrases (spaCy)
.small123[

```python
import spacy

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Example sentence
sentence = "Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest."

# Process the sentence with spaCy
doc = nlp(sentence)

# Extract noun phrases and verb phrases
noun_phrases = [chunk.text for chunk in doc.noun_chunks]
verb_phrases = [token.text for token in doc if token.pos_ == 'VERB']

# Print the results
print("Noun Phrases:", noun_phrases)
```

```
## Noun Phrases: ['Sarah', 'the research paper', 'that', 'highlighted scientists', 'a new species', 'the Amazon rainforest']
```

```python
print("Verb Phrases:", verb_phrases)
```

```
## Verb Phrases: ['mentioned', 'discovered']
```
]

---
# Noun phrase chunking
![NounPhraseChunking](https://www.nltk.org/book/tree_images/ch07-tree-1.png)
.small123[
*From NLTK ch. 2.1*
]

---
# Named Entity Recognition (NER)

.pull-left-narrow[
.small123[
- **Definition**
  - *Identifying and classifying named entities in text*
- **Named Entities**
  - *Entities such as names, locations, organizations, etc.*
- **Importance**
  - *Extracting structured information from unstructured text*
- **Applications**
  - *Information retrieval, question answering, and more*
]
]

.pull-right-wide[
.red[
***NER enhances information extraction by identifying and classifying entities in text.***
]
]
---
# NER with NLTK

```python
from nltk import ne_chunk
# Named Entity Recognition (NER) with NLTK
ner_sentence = "Apple Inc. is located in Cupertino, California."
ner_tokens = word_tokenize(ner_sentence)
ner_pos_tags = pos_tag(ner_tokens, tagset='universal')
ner_tree = ne_chunk(ner_pos_tags)

print("\nNER Example:")
```

```
## 
## NER Example:
```

```python
print(ner_tree)
```

```
## (S
##   Apple/NOUN
##   Inc./NOUN
##   is/VERB
##   located/VERB
##   in/ADP
##   Cupertino/NOUN
##   ,/.
##   (GPE California/NOUN)
##   ./.)
```

---
# NER with spaCy

**spaCy Code**

```python
import spacy
nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple Inc. is located in Cupertino, California.")
for ent in doc.ents:
    print(f"{ent.text} : {ent.label_} : {ent.label_}")
```

```
## Apple Inc. : ORG : ORG
## Cupertino : GPE : GPE
## California : GPE : GPE
```
.red[
***spaCy streamlines NER with its pre-trained models, providing accurate entity recognition.***
]

---
# NER visualised

```python
import spacy
from spacy import displacy

text = "When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously."

nlp = spacy.load("en_core_web_sm")
doc = nlp(text)
displacy.render(doc, style="ent")
```

'&lt;div class="entities" style="line-height: 2.5; direction: ltr"&gt;When \n&lt;mark class="entity" style="background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;"&gt;\n    Sebastian Thrun\n    &lt;span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem"&gt;PERSON&lt;/span&gt;\n&lt;/mark&gt;\n started working on self-driving cars at \n&lt;mark class="entity" style="background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;"&gt;\n    Google\n    &lt;span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem"&gt;ORG&lt;/span&gt;\n&lt;/mark&gt;\n in \n&lt;mark class="entity" style="background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;"&gt;\n    2007\n    &lt;span style="font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem"&gt;DATE&lt;/span&gt;\n&lt;/mark&gt;\n, few people outside of the company took him seriously.&lt;/div&gt;'

---
.pull-left-narrow[
## Reading into the past
- Topography (Geographical encyclopedia)
- Contains a lot of information hidden in text
- ~2000 parishes: High spatial resolution
]

.footnote[
.small123[
**Source** master's thesis by Andreas S. Ravnholt (2021) "Reading into the past: A method for Digitalising Historical Documents using NER"
]

]

.pull-right-wide[
.small123[
.panelset[
.panel[.panel-name[Front page]
![Trap](Figures/Trap.png)
]
.panel[.panel-name[NER]
![TrapNER](Figures/TrapNER.PNG)
]
.panel[.panel-name[Stats]
![TrapNER](Figures/TrapNER2.PNG)
]
.panel[.panel-name[Geography]
![TrapNER](Figures/Geography.PNG)
]

]
]
]



---
# Resolving the motivating example

.panelset[
.panel[.panel-name[Text]
.small123[
&gt; a. *The scientists discovered a new species in the Amazon rainforest*  
&gt; b. *The research paper highlighted that scientists discovered a new species in the Amazon rainforest*  
&gt; c. *Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest*  
&gt; d. *I believe Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest*
]

]
.panel[.panel-name[Code 1]
.small123[

```python
import spacy

# Load spaCy English model
nlp = spacy.load("en_core_web_sm")

# Example sentences
example_sentences = [
    "The scientists discovered a new species in the Amazon rainforest.",
    "The research paper highlighted that scientists discovered a new species in the Amazon rainforest.",
    "Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest.",
    "I believe Sarah mentioned the research paper that highlighted scientists discovered a new species in the Amazon rainforest."
]
```
]
]
.panel[.panel-name[Code 2]

```python
# Analyze each sentence with spaCy
for sentence in example_sentences:
    doc = nlp(sentence)
    # Extract Verb Phrases (VP) using spaCy's dependency parsing
    verb_phrases = [token.text for token in doc if token.pos_ == 'VERB']
    print(f"---&gt;Verb Phrases: {verb_phrases}")
```

```
## ---&gt;Verb Phrases: ['discovered']
## ---&gt;Verb Phrases: ['highlighted', 'discovered']
## ---&gt;Verb Phrases: ['mentioned', 'discovered']
## ---&gt;Verb Phrases: ['believe', 'mentioned', 'discovered']
```
]
]


---
# Code example
[Lecture 5 - Understanding and utilizing grammar/Code/Extracting_news_information.py](https://raw.githubusercontent.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%205%20-%20Understanding%20and%20utilizing%20grammar/Code/Extracting_news_information.py)  

[Lecture 5 - Understanding and utilizing grammar/Code/Getting_recent_news.py](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%205%20-%20Understanding%20and%20utilizing%20grammar/Code/Getting_recent_news.py)

---
# Efficient Market Hypothesis (EMH)

.pull-left[
*If you can buy cheaply something which is more valuable than its price, you can profit (arbitrage). But ...*
1. We all have access to the same information 
2. Everyone is interested in maximum profits
3. Everyone buys the undervalued assets 
4. Prices increases until the advantages is gone 

]

.pull-right[
![Efficient Market](Figures/stonks.jpeg)


### Implications:
- If any 'whole' in the market occurs it is closed within milliseconds 
- We cannot invest better than anyone else 
- Investing in what a chicken poops on is better than listening to an 'expert'
]

---

# Information Extraction in Financial Markets

.pull-left[
- The NLP promise is to be able to extract information 
- News about firms changes the expected returns on assets 
- [NVIDIA News](https://youtu.be/U0dHDr0WFmQ?si=EKAQgBiTekQhxl0u)
- If the expected returns on an asset increases or decreases there is an opportunity for arbitrage if prices have not changed
- NLP tools potentially allow us to be ahead 
- EMH dissallows open source implementations: You need to improve on the methods you get here
- **Timing is key**
]

.pull-right[
![Information Extraction](Figures/big_stonks.jpg)
]

---

# Volatility and Expected Returns Trade-off

.pull-left[
- Investors often face a trade-off between volatility and expected returns.
- Higher expected returns typically come with higher volatility.
- Investors need to balance risk and reward based on their risk tolerance and investment goals.
- Understanding market efficiency, extracting valuable information, and managing volatility are key elements.
- Investors in up till '08 thought they had diversified the risks out. 
]

.pull-right[
![Volatility and Returns](Figures/volatile_stonks.jpg)
]


---
class: inverse, middle
# Coding challenge: 
## 'Predicting volatility'
[Click here to submit](https://forms.gle/WmSEkZn8WH1fiDjE6 )

**Find this weeks coding challenge here:**
News_and_Market_Sentiment_Analytics/Coding_challenge/Week 5

---

# Next time
.pull-left[
- Embeddings
- Numerical representations of language 
- Distance and divergence measures
]

.pull-right[
![Trees](Figures/Trees.jpg)
]


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(SDU_logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 125px;
  height: 60px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // insert more to hide here
    ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
