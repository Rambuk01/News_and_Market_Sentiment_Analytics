---
title: "News and Market Sentiment Analytics"
subtitle: "Lecture 1: Context and setup"
author: 'Christian Vedel,<br> Department of Economics<br><br>
Email: christian-vs@sam.sdu.dk'
date: "Updated `r Sys.Date()`" 
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.tiny123 {
  font-size: 0.40em;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
```

# Today's lecture
.pull-left[
- Course overview
- How did we get to ChatGPT? (And what are the implications)
- An example of some research I do using the techniques of the course
]

.pull-right[
![Illustration](Figures/Chearth.jpg)
]

---
# Motivaiton for the course 

.pull-left[
- *It's all about information*
- Text is data 
- Applications: Business, Finance, Research, Software Development, Psychology, Linguistics, Business Intelligence, Policy Evaluation 
- In a world of LLMs you want to understand what is happening behind the fancy interface 
]

.pull-right[
![Counterexample](Figures/machine_learning.png)
*Counter-example to what we will be doing*
*(xkcd.com/1838)*
]

---
# About me - Christian Vedel
.pull-left[
- Economic Historian - freshly minted PhD 
- Research at the intersection between machine learning and economic history 
- A genuine interest in language, linguistics and etymology  
  + All though you will find my texts riddled with typos and grammatical errors 
- Write me emails
  + .red[christian-vs@sam.sdu.dk]
- We can make office hours if you want it. 

[Who are you? https://forms.gle/4T3Cjr42jwbx6vHHA](https://forms.gle/4T3Cjr42jwbx6vHHA)

]

.pull-right[
![Thesis](Figures/Defence.jpeg)
.small123[*PhD defence, 2023-09-25*]
]

---
# What to expect
- **Structure of the lectures**
  + 2x45 min. of lecture 
  + 40 + 5 min. of coding challenge
- **Book:**
  + We will use NLTK Natural Language Processing with Python by Ewan Klein, Steven Bird and Edward Loper https://www.nltk.org/book/
- **Exam:**
  + One week project in January (intended to be challenging) 
- **Course content:**
  + Low level NLP tools + applications 
  + Transformers $\rightarrow$ Basics $\rightarrow$ Transformers
- **Course plan:**
  + Continously updated on itslearning 
- **Slides:**
  + The night before the lecture 
  + Will not cover everything I say. Please bring a notepad and a pen.
- **Course description:**
  + https://odin.sdu.dk/sitecore/index.php?a=searchfagbesk&bbcourseid=N340076101-f-E23&lang=en 
  + Course plan on itslearning
  + [GitHub](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics)
  
---
# Preperation
.pull-left[
- Read assigned NLTK chapter. 
- Run code line by line while doing so.
- Make sure you understand what is happening. 
- Please go off tangents. Tangents are the best - if not for this specific course.

**Coding challenge:**
- Requires that you have tried the code in preparation 
- Will be challenging 
- Mistakes are great
]

.pull-right[
![Desk](Figures/Desk.jpg)
]

---
# Course plan
.small123[
| Date       | Slides | Assigned Reading* |
|------------|--------|-------------------|
| 2023-10-24 | [Lecture 1 - Context and setup](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%201%20-%20Context%20and%20setup/Slides.html)    | [itslearning]    |
| 2023-10-31 | [Lecture 2 - Lexical resources](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%202%20-%20Lexical%20resources/Slides.html) | NLTK ch. 2, ch. 3 ([NLTK Book](https://www.nltk.org/book/)), [The Zipf Mystery](https://youtu.be/fCn8zs912OE?si=NptILXURFS4WaFnw ) |
| 2023-11-07 | [Lecture 3 - Classification pt 1](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%203%20-%20Classification%20pt%201/Slides.html) | NLTK ch. 4, ch. 5 ([NLTK Book](https://www.nltk.org/book/)) |
| 2023-11-14 | [Lecture 4 - Classification pt 2](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%204%20-%20Classification%20pt%202/Slides.html) | NLTK ch. 5, ch. 6 ([NLTK Book](https://www.nltk.org/book/)), [Towards Data Science Article](https://towardsdatascience.com/introduction-to-embedding-clustering-and-similarity-11dd80b00061) |
| 2023-11-21 | [Lecture 5 - Understanding and utilizing grammar](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%205%20-%20Understanding%20and%20utilizing%20grammar/Slides.html) | Entity recognition article, NLTK ch. 7, ch. 8 ([NLTK Book](https://www.nltk.org/book/)) |
| 2023-11-28 | [Lecture 6 - The meaning of sentences](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%206%20-%20The%20meaning%20of%20sentences/Slides.html) | NLTK ch. 9, ch. 10 ([NLTK Book](https://www.nltk.org/book/)) |
| 2023-12-05 | [Lecture 7 - Sentiment analysis](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%207%20-%20Sentiment%20analysis/Slides.html) | Reading follows |
| 2023-12-12 | [Lecture 8 - Remaining topics](https://raw.githack.com/christianvedels/News_and_Market_Sentiment_Analytics/main/Lecture%208%20-%20Remaining%20topics/Slides.html#1) | Reading follows |
]

.footnote[
.small123[\* Expect this to change]
]

  
---
class: inverse, middle, center

# How did we get to ChatGPT?

---
# The promise of AI
.pull-left[
- The idea of the mechanic 'nightingale' ([HC] Andersen, 1844)
  + **Idea:** The mechanic version can never be as good because it is mechanic 
  + Reaches far back e.g. Descartes (1637) writes about automata in 'Discourse and Methods' 
- Can machines things? The idea of the Turing test (1950/60s)
  + If machines can think, they will respond convincingly in conversation 

]

.pull-right[
![Nightingale](Figures/The_mechanic_nightingale.png)
*Generated in beta.dreamstudio.ai*
]

---
# Early rule-based systems
.pull-left[
- We can model behaviour by *if then* statements 
- ELIZA (Weizenbaum, 1966):
  + The earliest convincing chatbot
  + Looks for keywords and otherwise writes something generic or repeats something from earlier
- 1980's was the peak of rule based systems 
  + Based on linguistic theory
  + Grammar 
  + Morphology
  + Semantics
]

.pull-right[
![ELIZA](Figures/ELIZA_conversation.png)
*ELIZA Conversation (Wikimedia Commons)* 
![Semantic](Figures/Semantic_Net.svg)
*Semantic net*
]

---
# The philosophical/linguistic problem
.pull-left[
- Can we distinguish reasoning from immitation? 
  + Chinese room problem (Seale, 1980) 
- What is the nature of language? 
  + Innateness vs. Empiricism (Chomsky vs. Vienna circle*)
  + Innateness $\rightarrow$ rule-based NLP 
  + Empiricism $\rightarrow$ 'empirical' (learning) methods 
- ChatGPT is a big score for the Empiricist hypothesis
]

.footnote[
\* E.g. (of names you might know) GÃ¶del, Popper, 
Reichenbach, Wittgenstein [(Wikipedia)](https://en.wikipedia.org/wiki/Vienna_Circle#Overview_of_the_members_of_the_Vienna_Circle)
]

.pull-right[
![Lightbulb](Figures/Light_bulb.jpg)
]

---
# 90s computer power - 60s maths
.pull-left[
- **Empiricist problem**: Statistics alone can only predict - not understand structure. Mathematical limitation (Pearl, 2019)
- But we can phrase language problems as predictions problems 
- Black box circumvents innateness 
- We just need to estimate: $$P(word|\{text\},\theta)$$
- E.g. "He is a [word]"
- Choose $\theta$ to minimize predicition errors
- Could be based on e.g. a [universal function approximator](https://en.wikipedia.org/wiki/Universal_approximation_theorem) like a neural network

]

.pull-right[
![DancingBaby](Figures/giphy.gif)   
.small123[*Crazy realistic 90s dancing baby*]

![NN](Figures/Architecture_small.png)
.small123[*Neural network*]

]


---
# History of Neural Networks for NLP 
.pull-left-wide[
- **'Perceptron'** neural network (Rosenblatt, .red[1958])
- **Long-Short-term memory** remembers and forgets previous words and letters (Hochreiter, Schmidhuber, .red[1997]) 
- A rule-based trick: **Word embeddings:** Queens, Kings, men, women (Firth, .red[1957] - But applied from .red[2000/2010s]; Roweis & Saul, .red[2000])
- **Tranformers**: 'Attention Is All You Need' (Vaswani, .red[2017])
- **Masked models**: E.g. BERT (Devlin et al, .red[2019])
  + "He is a [MASK] dog." $\rightarrow$  "He is a [smelly] dog."
- **GPT**: Radford & Narasimhan (.red[2018]): What if we make models really large? E.g. 175 billion parameters and train it on 499 Billion tokens (GPT-3)
- **What is a good answer?**: ChatGPT etc. (.red[2022]) trains a sepperate neural network to tell good from bad answers - reinforcement learning.
- **Hugginface**: Sharing of various transformer models.
]

.pull-right-narrow[
![Trees](Figures/Trees.jpg)
]

---
# Open questions
.pull-left[
- Do language models think? 
- What is consciousness? 
- Should LLMs be allowed for exams?
  + (Please use it for this one!) 
  + [Link to SDU's policy](https://sdunet.dk/en/undervisning-og-eksamen/nyheder/2023/0929_aiie23)

**What do you think?**
]

.pull-right[
**Evidence of structural learning**
![GT2023_1](Figures/Gurnee_Tegmark_2023_1.PNG)
![GT2023_2](Figures/Gurnee_Tegmark_2023_2.PNG)
.small123[
*Figure 1 from Gurnee & Tegmark (2023)*
]
]

---
# Setup
.pull-left[
- A lot of nltk 
- Also spaCy and transformers 
- Resource: [Natural Language Processing with spaCy & Python - Course for Beginners](https://www.youtube.com/watch?v=dIUTsFT2MeQ) + [spacy.io/usage](https://spacy.io/usage)

#### Conda environment

```{r eval=FALSE, include=TRUE}
conda create --name sentimentF23
conda activate sentimentF23
conda install spyder
conda install nltk
conda install numpy matplotlib pandas seaborn requests
```

]

.pull-right[

#### NLTK data
Download everything with the following python code    
```{r eval=FALSE, include=TRUE}
import nltk
nltk.download()
```

#### Install pytorch to run on cuda 11.8
```{r eval=FALSE, include=TRUE}
pip install transformers
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
```
 
]


.footnote[
.small123[More details in: [Setting-up-Python-environment.md](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Setting-up-Python-environment.md)]
]

---
# Before next time
.pull-left[
- Read chapter 2 and 3


- Run code from the chapters
  + Does it work?
  + How does it work?
  
  
- Watch ['The Zipf Mystery'](https://youtu.be/fCn8zs912OE?si=xVMA63kt9M99Qvjx)
]


.pull-right[
![Trees](Figures/Trees1.jpg)
]

---
## References (1/2)

.small123[
Andersen, H. C. (1844). The Nightingale. https://www.hcandersen-homepage.dk/?page_id=2257

Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171â4186, Minneapolis, Minnesota. Association for Computational Linguistics.

Descartes, R. (1637). Discourse on the Method of Rightly Conducting One's Reason and of Seeking Truth in the Sciences (J. Veitch, Trans.). EBook-No. 59. https://www.gutenberg.org/ebooks/59

Firth, J.R. (1957). "A synopsis of linguistic theory 1930â1955". Studies in Linguistic Analysis: 1â32. Reprinted in F.R. Palmer, ed. (1968). Selected Papers of J.R. Firth 1952â1959. London: Longman.

Gurnee, W., & Tegmark, M. (2023). Language Models Represent Space and Time. https://arxiv.org/abs/2310.02207

Pearl, J. (2019). The seven tools of causal inference, with reflections on machine learning. Communications of the ACM, 62(3), 54â60. https://doi.org/10.1145/3241036

Radford, A., & Narasimhan, K. (2018). Improving Language Understanding by Generative Pre-Training. https://api.semanticscholar.org/CorpusID:49313245

Rosenblatt, F. (1958). The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Review, 65(6), 386â408. https://doi.org/10.1037/h0042519
]


---
## References (2/2)
.small123[
Roweis, S. T., Saul, L. K. (2000). "Nonlinear Dimensionality Reduction by Locally Linear Embedding". Science, 290(5500), 2323â6. https://doi.org/10.1126/science.290.5500.2323

Searle, J. (1980), "Minds, Brains and Programs", Behavioral and Brain Sciences, 3(3), 417â457. https://doi.org/10.1017/S0140525X00005756

Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). Attention Is All You Need. https://arxiv.org/abs/1706.03762

Weizenbaum, J. (1966). ELIZAâa Computer Program for the Study of Natural Language Communication between Man and Machine. Commun. ACM, 9(1), 36â45. https://doi.org/10.1145/365153.365168
]
