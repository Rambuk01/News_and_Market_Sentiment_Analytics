---
title: "News and Market Sentiment Analytics"
subtitle: "Lecture 4: Classification pt 2"
author: "Christian Vedel,<br> Department of Economics<br><br>
Email: christian-vs@sam.sdu.dk"
date: "Updated `r Sys.Date()`" 
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=TRUE, include=TRUE, cache=TRUE)
library(reticulate)
use_condaenv("sentimentF23")
```


```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
```

# Last time
.pull-left[
- Standard classification approaches
- Extracting features using NLP tools 
- Training data $\rightarrow$ prediction engine
- **Research example:** Automatic occupational classification
]

.pull-right-narrow[
![Trees](Figures/Trees.jpg)
]

---
# Today's lecture
.pull-left[
- SoTA text classification in news and market sentiments analytics
- Off-the shelf models
- Transfer learning
]

.pull-right-narrow[
![Flows](Figures/0_Transformations, dataflows, magical, mathematical _esrgan-v1-x2plus.png)
]

---
class: middle
<div style="display: flex; align-items: center;">
    <img src="https://huggingface.co/favicon.ico" alt="HuggingFace Logo" style="height: 40px; margin-right: 10px;">
    <h1 style="margin: 0;">HuggingFace</h1>
</div>

.pull-left[
- HuggingFace is a platform for sharing machine learning models
- Most of the most advanced pretrained models are available here
- You can use the *off the shelf* or finetune them. Plenty of guidance available
]


---
class: middle

.pull-left-wide[
### Exercise
- Try navigating to [HuggingFace.co](https://huggingface.co/)
- Look for a model for one of the following tasks: Image Classification, Text Classification, Zero-Shot Classificaiton, Translation, Sentence Similarity
- Take a picture of yourself (or some on the internet) and try this tool: [huggingface.co/spaces/schibsted-presplit/facial_expression_classifier](https://huggingface.co/spaces/schibsted-presplit/facial_expression_classifier)
]


---
class: middle

## `pipeline`
- You can interact with many of these models using the `pipeline` interface of the `transformers` library
- You make an *instance* of a model buy calling `pipeline(task = "...")`
- You can find an overview of tasks here: [huggingface.co/docs/transformers/main/en/quicktour#pipeline](https://huggingface.co/docs/transformers/main/en/quicktour#pipeline)

---
class: middle

# Demo 1: Pictures from the news

.pull-left-wide[
- Automatic image description
]

.pull-right-narrow[
![Huang_Jensen](Figures/Jensen_Huang_20231109.jpg)
.small123[
Jensen Huang (CEO, NVIDIA) [picture from wikidmedia](https://upload.wikimedia.org/wikipedia/commons/3/36/Jensen_Huang_20231109_%28cropped2%29.jpg)
]
]

---
class: middle

# Demo 1: Auto caption

.pull-left-wide[
```{python, messages=FALSE}
from transformers import pipeline
model = pipeline(
    task="image-to-text",
    model = "Salesforce/blip-image-captioning-base"
)

results = model("Figures/Jensen_Huang_20231109.jpg")
```

```{python}
print(results)
```
]

.pull-right-narrow[
![Huang_Jensen](Figures/Jensen_Huang_20231109.jpg)
.small123[
Jensen Huang (CEO, NVIDIA) [picture from wikidmedia](https://upload.wikimedia.org/wikipedia/commons/3/36/Jensen_Huang_20231109_%28cropped2%29.jpg)
]
]

---
class: middle
# Demo 2: Sentiment analysis

```{python, messages=FALSE}
from transformers import pipeline
classifier = pipeline(
    task = "sentiment-analysis", 
    model = "distilbert/distilbert-base-uncased-finetuned-sst-2-english"
)
result = classifier("It is a lot of fun to learn advanced ML stuff.")
```

```{python}
print(result)
```

---
class: middle
# Demo 3: Named Entity Recognition

```{python}
from transformers import pipeline
model = pipeline(task = "ner", model = "dslim/bert-base-NER")
result = model(
    """
    Mr. Edward Jameson, born on October 15, 1867, in Concord, Massachusetts, 
    is a gentleman of notable accomplishments. He received his early 
    education at the local academy and graduated with honors from Harvard 
    College in 1888. Pursuing a career in law, he quickly rose to prominence 
    through his sagacity and dedication."
    """
)
```

```{python}
print(result)
```

---
class: middle
# Demo 4: Text generation from LLM

```{python, eval=FALSE}
from transformers import pipeline
import torch

model = pipeline(
    task = "text-generation", 
    model = 'HuggingFaceH4/zephyr-7b-beta', 
    torch_dtype=torch.bfloat16, 
    device_map='auto',
    )
result = model(
    "The practical aspects of data science", 
    max_length=50, 
    truncation=True,
    )
```

```{python, eval=FALSE}
print(result)
```

---
class: middle
# Demo 5: Zero-shot classification

**Example from last time**
```{python, cache = TRUE}
from transformers import pipeline

# Function for zero-shot classification
def zero_shot_classification(text, labels):
    classifier = pipeline("zero-shot-classification", model = "facebook/bart-large-mnli")
    result = classifier(text, labels)
    return {label: score for label, score in zip(result["labels"], result["scores"])}

# Example usage
text = "The movie was captivating and full of suspense."
labels = ["exciting", "dull", "intellectual"]
print(zero_shot_classification(text, labels))
```

---
class: middle

# Two approaches
.pull-left[
## Off-the-shelf
- *We can use models directly from e.g. HuggingFace*

#### Advantages
- Fast
- Often works well

#### Drawback
- Performance loss
- Poor performance in niche tasks
]

.pull-right[
## Transerlearning
- *We can adapt models to out needs*

#### Advantages
- Still relativly fast
- Can work surprisingly well

#### Drawback
- More code, more problems
- More fudging needed to get a solution to work
- Training can be expsensive (time/server/etc)
]

---
# Transferlearning

- The idea
- The transformer solution to finetuning
- Bulding a PyTorch model with BERT inside 
- To freeze or not to freeze? 

---
class: middle

# The Transformer Solution to Fine-tuning

.pull-left[
- Transformers are particularly well-suited for transfer learning
- Models like BERT, GPT, and T5 are pre-trained on large text corpora and can be fine-tuned for specific tasks
- Fine-tuning usually involves:
  + Modifying the modelâ€™s output layers
  + Training on your domain-specific data

]

---
# Building a PyTorch Model with BERT Inside

.pull-left[

1. **Load Pre-trained BERT Model**:
   - We use `transformers` to load a pre-trained model like BERT.
   
2. **Modify Output Layer**:
   - The output layer is adjusted for specific tasks (e.g., classification, named entity recognition).

3. **Fine-tuning**:
   - We fine-tune the model on our labeled data.
   
4. **Training Loop**:
   - Set up a PyTorch training loop for optimization.
]


```{python, eval=FALSE}
from transformers import BertForSequenceClassification, BertTokenizer
import torch
from torch.utils.data import DataLoader

# Load pre-trained BERT
model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

# Tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Example input text
text = "HuggingFace is an incredible resource for NLP."

# Tokenize input
inputs = tokenizer(text, return_tensors="pt")

# Forward pass
outputs = model(**inputs)
```


---
# The 'manual' way in `PyTorch`

.small123[
```{python, eval=FALSE}
import torch
import torch.nn as nn
from transformers import BertModel, BertConfig

class BERTClassifier(nn.Module):
    # Constructor class
    def __init__(self, config):
        super().__init__()
        
        # Load pre-trained BERT model
        self.basemodel = BertModel.from_pretrained(config["model_name"])
        
        # Dropout layer for regularization
        self.drop = nn.Dropout(p=config["dropout_rate"])
        
        # Output layer
        self.out = nn.Linear(self.basemodel.config.hidden_size, config["n_classes"])

    # CONTINUED ON NEXT PAGE ...
```
]


---
class: middle

.small123[
```{python, eval=FALSE}
    # Forward propagation method
    def forward(self, input_ids, attention_mask):
        # Pass input through BERT model
        outputs = self.basemodel(
            input_ids=input_ids,
            attention_mask=attention_mask
        )
        
        # Take the pooled output from the last layer of BERT
        pooled_output = outputs.pooler_output
        
        # Apply dropout
        output = self.drop(pooled_output)

        # Pass through the output layer to get logits
        return self.out(output)
```

]


---
class: middle

# To Freeze or Not to Freeze?

.pull-left[
### Freezing Layers:
- Freezing layers means not updating the weights during training.
- Can speed up training by reducing the number of parameters to optimize.
- Helps prevent overfitting, especially on small datasets.

### Unfreezing Layers:
- Unfreezing some or all layers allows the model to adjust more, improving performance for specific tasks.
- Typically results in better performance when enough labeled data is available.
]

.pull-right[
### Best Practice:
- Fine-tune the last few layers first to adapt the model to the new task.
- Gradually unfreeze more layers if necessary, allowing the model to learn more complex features.
]

---
# Freezing and unfreezing
```{python, eval=FALSE}
# First, freeze all layers
for param in model.basemodel.parameters():
    param.requires_grad = False
```

```{python, eval=FALSE}
# Gradually unfreeze layers in stages (e.g., unfreeze the last n layers)
def unfreeze_layers(model, num_layers_to_unfreeze=1):
    # Unfreeze the last `num_layers_to_unfreeze` layers of BERT
    total_layers = len(model.basemodel.encoder.layer)
    
    # Unfreeze from the last layer to the specified number of layers
    for i in range(total_layers - num_layers_to_unfreeze, total_layers):
        for param in model.basemodel.encoder.layer[i].parameters():
            param.requires_grad = True
```

```{python, eval=FALSE}
# Example: Unfreeze the last 2 layers
unfreeze_layers(model, num_layers_to_unfreeze=2)
```


---
class: middle

# Plan for the rest of the semester

.pull-left-wide[
- We need to talk about topics for the last bit of the course 
- Suggestions:
  + Topic modelling
  + RAGs
  + Linking
- Extra lecture: What would help you during the exam? 
]

---
class: middle

# Exam

.pull-left-wide[
- The exam is a one week assignment
- During this week, your task is to demonstrate, that you have picked up tools from this course by solving an assignment 
- It's very open ended.
- **Duration:** Between December 11th, 15:00 and December 18th, 15:00
- More info: [Exam/Format_of_the_exam.md](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Exam/Format_of_the_exam.md)
]

---
class: inverse, middle
# Coding challenge: 
## [Building a sentiment classifier](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%204%20-%20Classification%20pt%202/Coding_challenge_lecture4.md)

---
# Next time
.pull-left[
- Nuts and bolts: Tokenization and embeddings
- Guest lecture: A case of creative use of model embeddings 
]

.pull-right[
![Trees](Figures/Trees.jpg)
]






