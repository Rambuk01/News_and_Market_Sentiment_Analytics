---
title: "News and Market Sentiment Analytics"
subtitle: "Lecture 3: Classification pt 1"
author: "Christian Vedel,<br> Department of Economics<br><br>
Email: christian-vs@sam.sdu.dk"
date: "Updated `r Sys.Date()`" 
output:
  xaringan::moon_reader:
    includes:
      after_body: insert-logo.html
    self_contained: false
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval=TRUE, include=TRUE, cache=FALSE)
library(reticulate)
use_condaenv("sentimentF23")
```

```{python include=FALSE}
import nltk
```

```{css echo=FALSE}
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
```

# Last time
.pull-left[
- Introduction to basic tools
- A lot of not so interesting things. But you can refer back to this. 

### What we covered:
- `str` and what you can do with it
- Text cleaning (dealing with strange characters, stopwords, etc.)
- Lexical resources (AFINN) 
- Classical sentiment analysis
- Coding challenge: Zipf"s law
]

.pull-right-narrow[
![Trees](Figures/Trees.jpg)
]

---
# Today"s lecture
.pull-left[
- Standard classification approaches
- Extracting features using NLP tools 
- Training data $\rightarrow$ prediction engine
- **Research example:** Automatic occupational classification
]

.pull-right-narrow[
![Flows](Figures/0_Transformations, dataflows, magical, mathematical _esrgan-v1-x2plus.png)
]

---
class: middle
# Features to extract 
.pull-left[
- Word endings 
- Sentiment 
- Word type 
- Count vectorizer 
- Transformers:
  + ZeroShot classifcations (next time)
  + Sentiments
  + Embeddings
- ...All the relevant things we will learn about in the course 
]

.pull-right[
![Simple](Figures/Simple_illustration.png)
]


---
# The Naïve Bayes classifier (1/2)
.pull-left-wide[
.small123[
- We can infer $P(Class|X)$ via $P(Class|X)$
- In the Naïve Bayes classifier, we assume features are conditionally independent given the class, simplifying the calculation: $P(X|Class) = P(x_1|Class) \cdot P(x_2|Class) \cdot \ldots$
- Simple and very widely used classifier 
- $X$ is the features 
  + $P(Class|x_i) = P(x|Class)P(Class)/P(x_i)$ (Bayes" law)
  + $P(x_i|Class)=\frac{1}{n}\sum_{j\in Class} 1[x_j]$
  + $P(Class)$ can be assumed or estimated; $P(x_i)$ can also be estimated
  
- Assumes independence between features

- We will walk over intuition in the following slides
]
]


.pull-right-narrow[
![Bayes](https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif)
.small123[*Maybe* Bayes, wikimedia commons]
]
  

.footnote[
.small[
1. [See 3Blue1Brown explaination of Bayes theorem](https://youtu.be/U_85TaXbeIo)
2. You can use libraries for this, but often this is makes more sense to implement manually 
]
]


---
# The Naïve Bayes classifier (2/2)

.small123[
- This and similar approaches used extensively for feature extraction and in social science research
  + The maths is simple and clear, which helps in Causal Inference applications 
  + We want to know the *kinds of errors* more rather than *minimizing the errors*
  + Sometimes works really well
- Two examples: 
  + Bentzen & Andersen (2022): More religious names are associated with less technological development
  + Bentzen, Boberg-Fazlic, Sharp, Skovsgaard, Vedel (2024): Certain Danish religious movements is associated with less integration into the US in 1800s. But this seemingly has little consequences for occupational outcomes.
  
- Always start with NB when dealing with textual data
- But do use other models as well :-)
]

---
class: middle
# NB intuition: Genders from names (1/5)

.pull-left-wide[
- We want to classify a name as either *male* or *female* 
- **Training data:** A list of of female / male names
- **Goal:** Use this data to produce a classifier
- **Note:** We do not need any fancy neural nets. We just need basic probability theory. 
]

---
class: middle
# NB intuition: Genders from names (2/5)
- Say we observe 20000 names - 10000 for each gender
- **What we observe:** The frequency of names among genders
  + "Carl" might appear 1023/10000 times as male and 3/10000 times as female in our data
  + "Carla" has a might appear 2148/10000 times as male and 5/10000 times as female in our data
  + Etc.
  
- **We can derive** an estimate of $Pr(Gender | Name)$:
  + $Pr(Name = \text{"Carl"} | Gender = \text{"Male"}) \approx 1023 / 10000 = 0.1023$
  + $Pr(Name = \text{"Carl"} | Gender = \text{"Female"}) \approx 3 / 10000 = 0.0003$
  + $Pr(Name = \text{"Carla"} | Gender = \text{"Male"}) \approx 5 / 10000 = 0.0005$
  + $Pr(Name = \text{"Carla"} | Gender = \text{"Female"}) \approx 2148 / 10000 = 0.2148$ 
  + *Repeat for all names and classes* 
- This is the "training" procedure for NB
  

---
class: middle
# NB intuition: Genders from names (3/5)

.pull-left-wide[
- **What we have:** An estimate of $Pr(Name | Gender)$
- **What we want:** An estimate of $Pr(Gender | Name)$

- **Solution:** Bayes" law: $Pr(Gender | Name) = \frac{Pr(Name | Gender) Pr(Gender)}{Pr(Name)}$
]

---
# NB intuition: Genders from names (4/5)

*We will calculate the probability that "Carla" is female*

--
#### 1. Assume $Pr(Gender = \text{"Female"}) = 0.5$ (Is this reasonable?)

--
#### 2. Calculate $Pr(Name = \text{"Carla"})$

--

$$Pr(Name) = \sum_{Gender} Pr(Name | Gender) Pr(Gender)$$

--

$$Pr(Name = \text{"Carla"}) = Pr(\text{"Carla"} | \text{"Female"}) Pr(\text{"Female"}) + Pr(\text{"Carla"} | \text{"Male"}) Pr(\text{"Male"})$$

--

$$Pr(Name = \text{"Carla"}) = 0.2148 \times 0.5 + 0.0005 \times 0.5$$

--

$$Pr(Name = \text{"Carla"}) = 0.10765$$

--
#### 3. Retrieve $Pr(Name = \text{"Carla"} | Gender = \text{"Female"}) = 0.2148$ from "training"


---
# NB intuition: Genders from names (5/5)

--
#### 4. Putting it all together

--

$$Pr(Gender = \text{"Female"} | Name = \text{"Carla"}) = \frac{Pr(\text{"Carla"} | \text{"Female"}) Pr(\text{"Female"})}{Pr(\text{"Carla"})}$$

--

$$Pr(Gender = \text{"Female"} | Name = \text{"Carla"}) = \frac{0.2148 \times 0.5}{0.10765}$$

--

$$Pr(Gender = \text{"Female"} | Name = \text{"Carla"}) = \frac{0.10740}{0.10765}$$


--
### Result:

$$Pr(Gender = \text{"Female"} | Name = \text{"Carla"}) = 0.998$$


*Note: Simple to automate and **fast** to compute*

---
# A real application 
.pull-left-wide[
- Census data can be the source of knowledge about long run development
- Census data does not always contain genders 
- We can use the gender information in names 
- Used via a Naive-Bayes classifier 
- This time using the entire name

- **Turns out:** We can obtain 97.2% accuracy with Naïve-Bayes
]

.pull-right-narrow[
![Census](https://upload.wikimedia.org/wikipedia/commons/9/9a/Folket%C3%A6lling-1840.jpg)
*Census data, 1840, wikimedia*
]


---
# Simple Naïve Bayes to identify gender
[Lecture 4 - Classification pt 2/Code/Gender_classification_all_Danish_names.py](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%204%20-%20Classification%20pt%202/Code/Gender_classification_all_Danish_names.py)

- Using modern name/gender data to build a classifier that works in 1787

---
# Simple Naïve Bayes to identify news categories
[Lecture 4 - Classification pt 2/Code/Classifying_news.py](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%204%20-%20Classification%20pt%202/Code/Classifying_news.py)

- Classifying categories of reuters

---
# Feature extraction 

### 1. Word type
```{python}
# Function to extract word type as a feature
def word_type_feature(word):
    pos_tag = nltk.pos_tag([word])[0][1]
    return {"word_type": pos_tag}

# Example usage
print(word_type_feature("dog"))
```

---
# Feature extraction 

### 2. Count vectorizer
```{python}
from sklearn.feature_extraction.text import CountVectorizer

# Function to extract count vectorizer features
def count_vectorizer_feature(corpus):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(corpus)
    feature_names = vectorizer.get_feature_names_out()
    return dict(zip(feature_names, X.toarray()[0]))

# Example usage
corpus = ["This is the first document.", "This document is the second document."]
print(count_vectorizer_feature(corpus))
```

---
# Feature extraction
### 3. Simple bag of words sentiment
```{python}
from afinn import Afinn

# Function to extract sentiment using AFINN score as a feature
def afinn_sentiment_feature(text):
    afinn = Afinn()
    sentiment_score = afinn.score(text)
    return {"afinn_sentiment": sentiment_score}

# Example usage
print(afinn_sentiment_feature("This was just great. Pretty amazing."))
```

---
# Feature extraction
### 4. Length of the Text
```{python}
# Function to extract text length as a feature
def text_length_feature(text):
    return {"text_length": len(text)}

# Example usage
print(text_length_feature("This is a short text."))

```

---
# Feature extraction
### 5: Number of Words
```{python}
# Function to extract number of words as a feature
def word_count_feature(text):
    words = nltk.word_tokenize(text)
    return {"word_count": len(words)}

# Example usage
print(word_count_feature("This is an example sentence."))
```

---
# Feature extraction
### 6: Average Word Length
```{python}
# Function to extract average word length as a feature
def avg_word_length_feature(text):
    words = nltk.word_tokenize(text)
    avg_length = sum(len(word) for word in words) / len(words) if len(words) > 0 else 0
    return {"avg_word_length": avg_length}

# Example usage
print(avg_word_length_feature("This is a demonstration of average word length."))

```

---
# Feature extraction
### 7: Presence of Specific Words
```{python}
# Function to check the presence of specific words as features
def specific_word_feature(text, target_words):
    features = {}
    for word in target_words:
        features[f"contains_{word}"] = (word in text.lower())
    return features

# Example usage
text = "This is a sample text with some specific words."
target_words = ["sample", "specific"]
print(specific_word_feature(text, target_words))

```

---
# Feature extraction
### 8: Zipf"s law features
```{python}
# ... Zipf coefficient from coding challenge 2
```


---
# Feature extraction 
### 9. ZeroShot Classification with Transformers
```{python, cache = TRUE}
from transformers import pipeline

# Function for zero-shot classification
def zero_shot_classification(text, labels):
    classifier = pipeline("zero-shot-classification")
    result = classifier(text, labels)
    return {label: score for label, score in zip(result["labels"], result["scores"])}

# Example usage
text = "The movie was captivating and full of suspense."
labels = ["exciting", "dull", "intellectual"]
print(zero_shot_classification(text, labels))
```

---
class: inverse, middle
# [Breaking the HISCO Barrier: AI and Occupational Data Standardization*](https://raw.githack.com/christianvedels/OccCANINE/refs/heads/dev/Project_dissemination/HISCO%20Slides/Slides.html) 
### *Conference presentation: An example of the kind of research I do with things you learn about in this course*

.footnote[
\* *it"s a link, that you can click!*
.small123[
[More details](https://raw.githack.com/christianvedels/Guest_Lectures_and_misc_talks/main/HISCO/Slides.html)
]
]

---
class: inverse, middle
# Coding challenge: 
## "Building a news classifier"
[Click here to submit](https://forms.gle/WmSEkZn8WH1fiDjE6 )

```{r echo=FALSE}
library(countdown)
source("../000_Misc_functions_for_slides.R")
vertical = 0.35
dist = 0.15

countdown(
  minutes = 35,
  seconds = 0,
  right = 0,
  top = ToPct(vertical),
  update_every = 10
)
# countdown(
#   minutes = 25,
#   seconds = 0,
#   right = 0,
#   top = ToPct(vertical + dist)
# )
```

---
# Next time
.pull-left[
- Utilizing grammar
- Entity recognition 
]

.pull-right[
![Trees](Figures/Trees.jpg)
]

---
# References
.small123[
Bentzen, J and L Andersen (2022), ‘DP16938 In the Name of God! Religiosity and the Transition to Modern Growth‘, CEPR Discussion Paper No. 16938. CEPR Press, Paris & London. https://cepr.org/publications/dp16938


Bentzen, J. S., Boberg-Fazlic, N., Sharp, P., Skovsgaard, C. V., & Vedel, C. (2023). Does Cultural Assimilation Matter? In C. Vedel (Ed.), Natural Experiments in Geography and Institutions: Essays in the Economic History of Denmark (Chapter 3). [Ph.D. thesis, SDU]. Syddansk Universitet. Det Samfundsvidenskabelige Fakultet. https://doi.org/10.21996/jt34-zc23
]

