<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>News and Market Sentiment Analytics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Christian Vedel,  Department of Economics   Email: christian-vs@sam.sdu.dk" />
    <script src="libs/header-attrs-2.21/header-attrs.js"></script>
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# News and Market Sentiment Analytics
]
.subtitle[
## Lecture 8: Remaining topics
]
.author[
### Christian Vedel,<br> Department of Economics<br><br> Email: <a href="mailto:christian-vs@sam.sdu.dk" class="email">christian-vs@sam.sdu.dk</a>
]
.date[
### Updated 2023-12-11
]

---


&lt;style type="text/css"&gt;
.pull-left {
  float: left;
  width: 44%;
}
.pull-right {
  float: right;
  width: 44%;
}
.pull-right ~ p {
  clear: both;
}


.pull-left-wide {
  float: left;
  width: 66%;
}
.pull-right-wide {
  float: right;
  width: 66%;
}
.pull-right-wide ~ p {
  clear: both;
}

.pull-left-narrow {
  float: left;
  width: 30%;
}
.pull-right-narrow {
  float: right;
  width: 30%;
}

.small123 {
  font-size: 0.80em;
}

.large123 {
  font-size: 2em;
}

.red {
  color: red
}
&lt;/style&gt;

# Last time
.pull-left[
- Four recent academic papers on sentiment analysis
- How to access LLaMA2
- Recommendations on sentiment analysis
]

.pull-right[
![Trees](Figures/Trees.jpg)
]

---
# Today's lecture
.pull-left[
- Topic modelling: 
  + Clustering , LDA, etc. 
]

.pull-right[
![Clustering](Figures/Clustering_dall-e.jpg)
]

---
# A pipeline for extracting information from a heterogenous stack of documents

1. Figure out which document it is
2. Extract Appropriate Information
3. Use Information for New Insights


Example: See [Dahl, Johansen, Sørensen, Westermann, Wittrock (2023)](/https://arxiv.org/abs/2102.03239)


### Exam relevance
- In the exam you will be given a source of text data
- And it would be great if you extracted relevant information from it

---
# Document Classification in the Clustering Pipeline

## 1. Figure out which document it is

- **Task Description:**
  + Identify the type or category of a given document.
- **Approach:**
  + Utilize document classification to assign predefined labels or categories.

![Dahl](Figures/Dahl et al 2023.png)
.small123[*Figure 1 from Dahl et al (2023)*]

---

## 2. Extract Appropriate Information

- **Task Description:**
  + Extract relevant information from the identified document.
- **Technique:**
  + Apply Named Entity Recognition (NER) to identify and extract entities.


---

## 3. Use Information for New Insights

- **Task Description:**
  + Leverage the extracted information to gain new insights or knowledge.
- **Application:**
  + Analyze entities, relationships, or patterns to derive meaningful insights.


---
# Topic Modeling 
.pull-left[
- A common issue when dealing with text, is that we want to extract topics 
- *This is an unsupervised learning task*
  + E.g. clustering 
- We can derive valuable clusters from the classical approach 'Bag of words' 
  + k means
  + Latent Diriclet Allocation
- Great results from more modern approaches
  + K means - just in embedding space
]

.pull-right[
![Topicmodelling](Figures/Topics.png)
]

---
# K Means
.pull-left[
- **Representation:**
  + **Bag of Words (BoW):** Each document represented by word frequencies.
- **Algorithm:**
  + K means leverages BoW for simple document similarity based on word occurrences.
- The simplest form of topic discovery. 
- Requires tinkering to get useful results
  
]

.pull-right[
![K-Nearest Neighbors](Figures/k_means.png)
]

---
# K-Means implementation
.pull-left[
.small123[
### Steps
- **Step 0:** Clean data, remove stopwords, etc.
- **Step 1:** BoW representaiton
  + Each document is represented as a vector of word frequencies 
- **Step 2 (optional):** Consider reducing the dimensionality of the problem
  + I.e. limit to only useful words using PCA or other heuristics 
- **Step 3:** Run K means for k in 1:K
  + Use some evaluation metric to choose optimal k. 
  + Metrics: Sum of squared errors
  + Criterion: Elbow point
  
**See [Lecture 8 - Remaining topics/Code/K_means_topic_modelling.py](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%208%20-%20Remaining%20topics/Code/K_means_topic_modelling.py)**
]

]

.pull-right[
.small123[
### Each K means run:
Init: Set k means `\(\{\mathbf{m}^{(k)}\}\)` randomly

1. **Assignment**
  + Measure distances `\(\{Dist\} = Dist\left(\mathbf{m}^{(k)},\mathbf{x}^{(n)}\right)\)` using e.g. Euclidean or Cosine distance
  + Assign all elements of `\(\mathbf{x}^{(k)}\)` to an element of `\(\mathbf{m}^{(k)}\)` for which `\(\{Dist\}\)` is minimal
  
2. **Update**
  + Updates all `\(\mathbf{m}^{(k)}\)` to be equal to the mean of its assigned elements of `\(\mathbf{x}^{(k)}\)`
  
- **Repeat 1 and 2 until convergence**
  
MacKay (2003), p. 286
]

]

---
# Latent Dirichlet Allocation (LDA)
.pull-left[
- We imagine that every document is generated in the BoW way
- But in two steps:
  1. Each document is a mixture of topics 
  2. Each topic is a mixture of words
- 1. and 2. are drawn from a Dirichlet distribution
- Topics are the latent part we are trying to identify
]

.pull-right[
![LDA](Figures/lda.png)
]

---
# Latent Dirichlet Allocation (LDA) Implementation
.pull-left[
.small123[
### Steps
- **Step 0:** Clean data, remove stopwords, etc.
- **Step 1:** Prepare the document-term matrix (DTM)
  + Each document is represented as a vector of word frequencies or other features
- **Step 2:** Run LDA for a given number of topics
  + Use some evaluation metric to choose the optimal number of topics.
  + Criterion: Elbow point
  
**See [Lecture 8 - Remaining topics/Code/LDA_topic_modelling.py](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%208%20-%20Remaining%20topics/Code/LDA_topic_modelling.py)**
]

]

.pull-right[
.small123[
### LDA estimation
**Hiearchical Bayesian model**
- `\(\theta_i\sim Dir(\alpha)\)`: Topic mixture distribution
- `\(\varphi_k\sim Dir(\beta)\)`: Word in topic k

To generate one document repeat:
1. Choose topic `\(z_{ij} \sim Multinomial(\theta_i)\)` 
2. Choose word from topic `\(w_{ij}\sim Multinomial(\varphi_{z_{ij}})\)`

Can be estimated with Markov Chain Monte Carlo. E.g. Gibbs sampling

*[What is a Dirichlet distribution? It's a distribution of distributions](https://en.wikipedia.org/wiki/Dirichlet_distribution)*

]

]

---
# Sentence-Level Transformers and Clustering
.pull-left[
- Leverage advanced transformer models for sentence embeddings.
- Understand context and semantics at a fine-grained level.
- Utilize these embeddings for document clustering in the embedding space.
]

.pull-right[
![Sentence-Level Transformers](Figures/embedding_space.png)
]

---
# Sentence-Level Transformer Embeddings
.pull-left[

### Approach
- Use transformer models designed for sentence embeddings (e.g., DistilBERT, RoBERTa).
- Sentence embeddings capture rich contextual information.
- Fine-tune or use pre-trained models for diverse downstream tasks.
- Create embeddings for each sentence in the dataset.

[Lecture 8 - Remaining topics/Code/Embedding_topic_modelling.py](https://github.com/christianvedels/News_and_Market_Sentiment_Analytics/blob/main/Lecture%208%20-%20Remaining%20topics/Code/Embedding_topic_modelling.py)

]

.pull-right[

### Steps
1. Embed the sentences
2. Reduce dimensions (PCA, t-SNE)
3. Run clustering algorithm (K-means)

]

---
class: center
# Course evaluation

.pull-left[
**Keep, Add, Drop (KAD)**
Online form
]

.pull-right[
![KAT](Figures/KAT.png)
*A 'KAD'*
]



---
class: center, middle
# Coding challenge: Research level NLP


---
## References

.small123[
Dahl, C. M., Johansen, T. S. D., Sørensen, E. N., Westermann, C. E., &amp; Wittrock, S. (2023). Applications of machine learning in tabular document digitisation. Historical Methods: A Journal of Quantitative and Interdisciplinary History, 56(1), 34-48. https://doi.org/10.1080/01615440.2023.2164879 (WP version: https://arxiv.org/abs/2102.03239)

MacKay, D. J. C. (2003). Information theory, inference, and learning algorithms. Cambridge University Press. https://www.inference.org.uk/itprnn/book.pdf

Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. (2003). Latent Dirichlet Allocation. Journal of Machine Learning Research, 3, 993-1022. http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf
]


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>
<style>
.logo {
  background-image: url(SDU_logo.png);
  background-size: contain;
  background-repeat: no-repeat;
  position: absolute;
  top: 1em;
  right: 1em;
  width: 125px;
  height: 60px;
  z-index: 0;
}
</style>

<script>
document
  .querySelectorAll(
    '.remark-slide-content' +
    ':not(.title-slide)' +
    // insert more to hide here
    ':not(.inverse)' +
    ':not(.hide-logo)'
  )
  .forEach(el => {
    el.innerHTML += '<div class="logo"></div>';
  });
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
